{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGyx11menMIJ",
        "outputId": "2dd19e2f-321c-483b-99fb-c5d4a3407d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "\n",
        "# tf-models-official is the stable Model Garden package\n",
        "# tf-models-nightly includes latest changes\n",
        "!pip install -U -q \"tf-models-official\"\n",
        "\n",
        "\n",
        "# Install the mediapy package for visualizing images/videos.\n",
        "# See https://github.com/google/mediapy\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run imports\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import PIL\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tqdm\n",
        "import absl.logging\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 10,\n",
        "})"
      ],
      "metadata": {
        "id": "7b5ESnwDnrmD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246c4JQVnmln",
        "outputId": "7f886f07-8964-4fca-d754-fddc3aca3e53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Source and destination paths\n",
        "source = '/content/drive/MyDrive/CarCrash'  # Replace with your actual source path\n",
        "destination = '/content/MyData'\n",
        "\n",
        "# Copy the directory\n",
        "shutil.copytree(source, destination,dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0aNE7BK7nm-E",
        "outputId": "19f2a342-0b13-40c0-baf6-d82df948e1da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/MyData'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/MyData/Crash-1500.zip'\n",
        "\n",
        "# Destination directory to extract\n",
        "extract_path = '/content/MyData2/Crash'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(zip_path):\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Unzipped '{zip_path}' to '{extract_path}' successfully.\")\n",
        "else:\n",
        "    print(f\"Zip file '{zip_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfBy54tPnoKk",
        "outputId": "01d73e2b-8e9f-4ea5-e0d6-7edd6db9f8e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped '/content/MyData/Crash-1500.zip' to '/content/MyData2/Crash' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/MyData/Normal.zip'\n",
        "\n",
        "# Destination directory to extract\n",
        "extract_path = '/content/MyData2/Normal'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(zip_path):\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Unzipped '{zip_path}' to '{extract_path}' successfully.\")\n",
        "else:\n",
        "    print(f\"Zip file '{zip_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSJTqfjx0DAr",
        "outputId": "eced5087-4728-4c2a-d76a-509526d762fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped '/content/MyData/Normal.zip' to '/content/MyData2/Normal' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Source folders\n",
        "source_crash = '/content/MyData2/Crash'\n",
        "source_normal = '/content/MyData2/Normal'\n",
        "\n",
        "# Destination folders\n",
        "train_crash = '/content/DATA/train/crash'\n",
        "train_normal = '/content/DATA/train/normal'\n",
        "val_crash = '/content/DATA/val/crash'\n",
        "val_normal = '/content/DATA/val/normal'\n",
        "test_crash = '/content/DATA/test/crash'\n",
        "test_normal = '/content/DATA/test/normal'\n",
        "\n",
        "# Create destination directories\n",
        "os.makedirs(train_crash, exist_ok=True)\n",
        "os.makedirs(train_normal, exist_ok=True)\n",
        "os.makedirs(val_crash, exist_ok=True)\n",
        "os.makedirs(val_normal, exist_ok=True)\n",
        "os.makedirs(test_crash, exist_ok=True)\n",
        "os.makedirs(test_normal, exist_ok=True)\n",
        "\n",
        "# Get list of all files in crash and normal folders\n",
        "crash_files = [os.path.join(source_crash, f) for f in os.listdir(source_crash) if f.endswith('.mp4')]\n",
        "normal_files = [os.path.join(source_normal, f) for f in os.listdir(source_normal) if f.endswith('.mp4')]\n",
        "\n",
        "# Split the data (70% train, 20% val, 10% test)\n",
        "train_crash_files, temp_crash_files = train_test_split(crash_files, test_size=0.3, random_state=42)\n",
        "val_crash_files, test_crash_files = train_test_split(temp_crash_files, test_size=1/3, random_state=42)\n",
        "\n",
        "train_normal_files, temp_normal_files = train_test_split(normal_files, test_size=0.3, random_state=42)\n",
        "val_normal_files, test_normal_files = train_test_split(temp_normal_files, test_size=1/3, random_state=42)\n",
        "\n",
        "# Function to copy files with tqdm for progress tracking\n",
        "def copy_files(file_list, destination_folder, desc):\n",
        "    for file in tqdm(file_list, desc=desc):\n",
        "        shutil.copy(file, destination_folder)\n",
        "\n",
        "# Copy crash files\n",
        "copy_files(train_crash_files, train_crash, \"Copying crash train files\")\n",
        "copy_files(val_crash_files, val_crash, \"Copying crash val files\")\n",
        "copy_files(test_crash_files, test_crash, \"Copying crash test files\")\n",
        "\n",
        "# Copy normal files\n",
        "copy_files(train_normal_files, train_normal, \"Copying normal train files\")\n",
        "copy_files(val_normal_files, val_normal, \"Copying normal val files\")\n",
        "copy_files(test_normal_files, test_normal, \"Copying normal test files\")\n",
        "\n",
        "print(\"Data split and copy completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmNvkVVDogPk",
        "outputId": "c8a56a4e-45ba-4e3c-c4b3-fe3c425a436f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying crash train files: 100%|██████████| 1050/1050 [00:00<00:00, 1745.23it/s]\n",
            "Copying crash val files: 100%|██████████| 300/300 [00:00<00:00, 715.02it/s]\n",
            "Copying crash test files: 100%|██████████| 150/150 [00:00<00:00, 965.93it/s]\n",
            "Copying normal train files: 100%|██████████| 2100/2100 [00:12<00:00, 163.99it/s]\n",
            "Copying normal val files: 100%|██████████| 600/600 [00:08<00:00, 72.21it/s]\n",
            "Copying normal test files: 100%|██████████| 300/300 [00:01<00:00, 261.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split and copy completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import PosixPath\n",
        "\n",
        "data_paths = {\n",
        "    'train': PosixPath('/content/DATA/train'),\n",
        "    'test': PosixPath('/content/DATA/test'),\n",
        "    'val': PosixPath('/content/DATA/val')\n",
        "}\n",
        "\n",
        "print(data_paths)\n",
        "print(data_paths['train'])\n",
        "print(data_paths['test'])\n",
        "subset_paths = data_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEm4O1WWtSnE",
        "outputId": "1eda04c7-9d3b-4002-ce22-f69c28ff0b95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': PosixPath('/content/DATA/train'), 'test': PosixPath('/content/DATA/test'), 'val': PosixPath('/content/DATA/val')}\n",
            "/content/DATA/train\n",
            "/content/DATA/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(subset_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq1dtCHxtxRM",
        "outputId": "a57db9d6-1c56-4dc1-9d42-9b62b76c39af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': PosixPath('/content/DATA/train'), 'test': PosixPath('/content/DATA/test')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (172,172), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=10)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label.\n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames.\n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.mp4'))\n",
        "    classes = [p.parent.name for p in video_paths]\n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames)\n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ],
      "metadata": {
        "id": "jD8_1Coptyvd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the directory containing the videos\n",
        "video_dir = '/content/DATA/test/crash'\n",
        "\n",
        "# Initialize variables to calculate the average frames\n",
        "total_frames = 0\n",
        "video_count = 0\n",
        "\n",
        "# Loop through all video files in the directory\n",
        "for video_file in os.listdir(video_dir):\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "\n",
        "    # Ensure the file is a video (e.g., .mp4)\n",
        "    if video_file.endswith('.mp4'):  # Adjust the extension if needed\n",
        "        video_count += 1\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get the number of frames in the video\n",
        "        frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        total_frames += frames\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "# Calculate the average number of frames\n",
        "if video_count > 0:\n",
        "    average_frames = total_frames / video_count\n",
        "    print(f\"Average frames per video: {average_frames:.2f}\")\n",
        "    print(f\"Total videos processed: {video_count}\")\n",
        "else:\n",
        "    print(\"No videos found in the directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5diN7Wnt0IF",
        "outputId": "75777365-4f73-43a4-d1ab-e8923af2dd9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average frames per video: 50.00\n",
            "Total videos processed: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_frames = 8\n",
        "\n",
        "CLASSES = sorted(os.listdir('/content/DATA/train'))\n",
        "print(CLASSES)\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
        "                                          output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], num_frames),\n",
        "                                          output_signature = output_signature)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
        "                                         output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my8fDhXCt28u",
        "outputId": "3f8f0c23-6204-4995-cbd1-0cb75b01ea33"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['crash', 'normal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model\n",
        "from official.projects.movinet.tools import export_saved_model"
      ],
      "metadata": {
        "id": "4fuV5KXD01sX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'a5'\n",
        "use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
        "resolution = 172\n",
        "\n",
        "backbone = movinet.Movinet(\n",
        "    model_id=model_id,\n",
        "    causal=True,\n",
        "    conv_type='2plus1d',\n",
        "    se_type='2plus3d',\n",
        "    activation='hard_swish',\n",
        "    gating_activation='hard_sigmoid',\n",
        "    use_positional_encoding=use_positional_encoding,\n",
        "    use_external_states=False,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "3rA__qXJvY0Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone,\n",
        "    num_classes=600,\n",
        "    output_states=True)\n",
        "\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a5_stream.tar.gz -O movinet_a5_stream.tar.gz -q\n",
        "!tar -xvf movinet_a5_stream.tar.gz\n",
        "\n",
        "checkpoint_dir = 'movinet_a5_stream'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU_ORrLlxLsX",
        "outputId": "00647d2e-3736-4f21-dd9e-56f5a991fe8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movinet_a5_stream/\n",
            "movinet_a5_stream/ckpt-1.data-00000-of-00001\n",
            "movinet_a5_stream/ckpt-1.index\n",
            "movinet_a5_stream/checkpoint\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f785d538eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu_resolver = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "\n",
        "# Select appropriate distribution strategy\n",
        "if tpu_resolver:\n",
        "  tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "  distribution_strategy = tf.distribute.experimental.TPUStrategy(tpu_resolver)\n",
        "  print('Running on TPU ', tpu_resolver.cluster_spec().as_dict()['worker'])\n",
        "elif len(gpus) > 1:\n",
        "  distribution_strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  distribution_strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  distribution_strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "\n",
        "print(\"Number of accelerators: \", distribution_strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOH6_M2cxXoM",
        "outputId": "86573e04-86e7-4dab-c84d-7730e7fbed26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on single GPU  /device:GPU:0\n",
            "Number of accelerators:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  # model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Construct loss, optimizer and compile the model\n",
        "with distribution_strategy.scope():\n",
        "  model = build_classifier(batch_size, num_frames, resolution, backbone, 2)\n",
        "  loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  model.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "iNgVAF1exbi_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/trained_model/cp.weights.h5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,  # Saves only the model's weights\n",
        "    save_best_only=False,    # If True, only saves the best weights\n",
        "    monitor='val_loss',      # Metric to monitor for `save_best_only`\n",
        "    mode='auto',             # Can be 'min', 'max', or 'auto'\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "JHtdNRMLxonO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "results = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=2,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1,\n",
        "                    )\n",
        "tf.saved_model.save(model, 'movinet_a0_saved_model4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ6INNjdxpOZ",
        "outputId": "792f683a-637f-4878-d254-e17390fc9ac8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "315/315 [==============================] - 699s 2s/step - loss: 0.1958 - accuracy: 0.9219 - val_loss: 0.2443 - val_accuracy: 0.9022\n",
            "Epoch 2/2\n",
            "315/315 [==============================] - 594s 2s/step - loss: 0.1178 - accuracy: 0.9568 - val_loss: 0.0458 - val_accuracy: 0.9844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(model, 'movinet_a0_saved_model4')"
      ],
      "metadata": {
        "id": "9Jybx2Tp3bHC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the folder to be zipped and the output zip file name\n",
        "folder_to_zip = '/content/movinet_a0_saved_model4'  # Replace with the path to your folder\n",
        "output_zip_file = '/content/model'  # Replace with desired zip file name\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "\n",
        "print(f\"Folder zipped successfully as {output_zip_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf3DZO4R-0kk",
        "outputId": "7ad9ee66-c95a-4e59-c0c4-c777027e1e3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder zipped successfully as /content/model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ],
      "metadata": {
        "id": "yVVDzMnmx29E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "metadata": {
        "id": "hrvz7d_cx3mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(6, 16)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "UW4XbVghx5XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())\n",
        ""
      ],
      "metadata": {
        "id": "KLbfhyoZx7Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ],
      "metadata": {
        "id": "DMisk_cQx_Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights to a checkpoint\n",
        "checkpoint_path = '/content/trained_model/movinet_weights.ckpt'\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.save(checkpoint_path)\n",
        "\n",
        "print(f\"Weights saved to {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpSgbm0LDQXb",
        "outputId": "dbd33a4b-c0f3-4310-9544-a924ffba51c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights saved to /content/trained_model/movinet_weights.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from official.projects.movinet.modeling import movinet, movinet_model\n",
        "\n",
        "# Model configuration\n",
        "model_id = 'a5'\n",
        "use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
        "resolution = 172\n",
        "\n",
        "# Create backbone and model\n",
        "backbone = movinet.Movinet(\n",
        "    model_id=model_id,\n",
        "    causal=True,\n",
        "    conv_type='2plus1d',\n",
        "    se_type='2plus3d',\n",
        "    activation='hard_swish',\n",
        "    gating_activation='hard_sigmoid',\n",
        "    use_positional_encoding=use_positional_encoding,\n",
        "    use_external_states=True,\n",
        ")\n",
        "\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone,\n",
        "    num_classes=2,\n",
        "    output_states=True\n",
        ")\n",
        "\n",
        "# Load weights using a checkpoint\n",
        "checkpoint_dir = '/content/trained_model'\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "if latest_checkpoint:\n",
        "    checkpoint.restore(latest_checkpoint).expect_partial()\n",
        "    print(f\"Weights restored from {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"No checkpoint found in the directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKoPe1xkyAGv",
        "outputId": "a10d77e6-5f27-435b-8803-54883ee56d0d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights restored from /content/trained_model/movinet_weights.ckpt-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k(probs, k=5, label_map=CLASSES):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))\n",
        ""
      ],
      "metadata": {
        "id": "BZiry5oVySTs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create initial states for the stream model\n",
        "init_states_fn = model.init_states\n",
        "init_states = init_states_fn(tf.shape(tf.ones(shape=[1, 1, 172, 172, 3])))\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for frames, label in test_ds.take(1):\n",
        "  for clip in frames[0]:\n",
        "    # Input shape: [1, 1, 172, 172, 3]\n",
        "    clip = tf.expand_dims(tf.expand_dims(clip, axis=0), axis=0)\n",
        "    logits, states = model.predict({**states, 'image': clip}, verbose=0)\n",
        "    all_logits.append(logits)\n",
        "\n",
        "logits = tf.concat(all_logits, 0)\n",
        "probs = tf.nn.softmax(logits)\n",
        "\n",
        "final_probs = probs[-1]\n",
        "top_k = get_top_k(final_probs)\n",
        "print()\n",
        "for label, prob in top_k:\n",
        "  print(label, prob)\n",
        "\n",
        "frames, label = list(test_ds.take(1))[0]\n",
        "to_gif(frames[0].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "UwQQfDzgyUI1",
        "outputId": "d37ad865-8d75-4af6-ac8c-af04b5d1d7c6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 5 is not in [0, 2) [Op:GatherV2]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-5e2919a80f7f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mfinal_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d37ce74e3cc8>\u001b[0m in \u001b[0;36mget_top_k\u001b[0;34m(probs, k, label_map)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtop_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DESCENDING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtop_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtop_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtop_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 5 is not in [0, 2) [Op:GatherV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Path to the saved model\n",
        "model_path = '/content/movinet_a0_saved_model4'\n",
        "\n",
        "# Load the model\n",
        "model = tf.saved_model.load(model_path)\n",
        "\n",
        "# Verify the model is loaded\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "CjTEcdV7A6a1",
        "outputId": "fb4ad36f-1825-4482-9ab8-397e9bae40a3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_UserObject' object has no attribute 'summary'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7ad75737d88c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Verify the model is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'summary'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.signatures)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juLlD71aCwr9",
        "outputId": "495dab0f-d61d-430f-fe2b-ef9c01ee212d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_SignatureMap({'serving_default': <ConcreteFunction (*, image: TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='image')) -> Dict[['classifier_head_2', TensorSpec(shape=(None, 2), dtype=tf.float32, name='classifier_head_2')]] at 0x7F714A65C220>})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = '/content/model'\n",
        "tflite_filename = 'model.tflite'\n",
        "input_shape = [1, 1, 172, 172, 3]\n",
        "\n",
        "# Convert to saved model\n",
        "export_saved_model.export_saved_model(\n",
        "    model=model,\n",
        "    input_shape=input_shape,\n",
        "    export_path=saved_model_dir,\n",
        "    causal=True,\n",
        "    bundle_input_init_states_fn=False\n",
        "    )"
      ],
      "metadata": {
        "id": "65VfqkYiyZLA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(tflite_filename, 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "# Create the interpreter and signature runner\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_filename)\n",
        "runner = interpreter.get_signature_runner()\n",
        "\n",
        "init_states = {\n",
        "    name: tf.zeros(x['shape'], dtype=x['dtype'])\n",
        "    for name, x in runner.get_input_details().items()\n",
        "}\n",
        "del init_states['image']"
      ],
      "metadata": {
        "id": "VCG3eQ5Myp5c"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for frames, label in test_ds.take(1):\n",
        "  for clip in frames[0]:\n",
        "    # Input shape: [1, 1, 172, 172, 3]\n",
        "    outputs = runner(**states, image=clip)\n",
        "    logits = outputs.pop('logits')[0]\n",
        "    states = outputs\n",
        "\n",
        "probs = tf.nn.softmax(logits)\n",
        "top_k = get_top_k(probs)\n",
        "print()\n",
        "for label, prob in top_k:\n",
        "  print(label, prob)\n",
        "\n",
        "frames, label = list(test_ds.take(1))[0]\n",
        "to_gif(frames[0].numpy())"
      ],
      "metadata": {
        "id": "vGFPdNkryr09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}